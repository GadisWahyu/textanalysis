{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Prepare Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install package from github\n",
    "pip install --upgrade https://github.com/JoMingyu/google-play-scraper/tarball/master #Google Play Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Sastrawi #NLP of Bahasa Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the library\n",
    "\n",
    "#Scraping\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pygments import highlight\n",
    "from pygments.lexers import JsonLexer\n",
    "from pygments.formatters import TerminalFormatter\n",
    "\n",
    "from google_play_scraper import Sort, reviews, app\n",
    "\n",
    "#Visualizing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "#Cleansing\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "\n",
    "#TF IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of the apps, put the words after id=\n",
    "app_packages = [\n",
    "  'com.tokopedia.tkpd',\n",
    "  'com.tokopedia.sellerapp',\n",
    "  'com.tokopedia.kelontongapp',\n",
    "  'com.shopee.id',\n",
    "  'com.shopee.mitra.id',\n",
    "  'com.lazada.android',\n",
    "  'com.sc.lazada',\n",
    "  'com.bukalapak.android',\n",
    "  'com.bukalapak.mitra',\n",
    "  'blibli.mobile.commerce',\n",
    "  'blibli.instore.mitra',\n",
    "  'com.gdn.blibli.mta',\n",
    "  'jd.cdyjy.overseas.market.indonesia',\n",
    "  'com.jdid.fans',\n",
    "  'id.jd.cn.seller',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_infos = []\n",
    "\n",
    "for ap in tqdm(app_packages):\n",
    "  info = app(ap, lang='id', country='id')\n",
    "  del info['comments']\n",
    "  app_infos.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_json(json_object):\n",
    "  json_str = json.dumps(\n",
    "    json_object,\n",
    "    indent=2,\n",
    "    sort_keys=True,\n",
    "    default=str\n",
    "  )\n",
    "  print(highlight(json_str, JsonLexer(), TerminalFormatter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_json(app_infos[0]) #App info of the first app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Icons of the apps\n",
    "\n",
    "def format_title(title):\n",
    "  sep_index = title.find(':') if title.find(':') != -1 else title.find('-')\n",
    "  if sep_index != -1:\n",
    "    title = title[:sep_index]\n",
    "  return title[:10]\n",
    "\n",
    "fig, axs = plt.subplots(3, len(app_infos) // 3, figsize=(14, 8))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "  ai = app_infos[i]\n",
    "  img = plt.imread(ai['icon'])\n",
    "  ax.imshow(img)\n",
    "  ax.set_title(format_title(ai['title']))\n",
    "  ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export App Infos to CSV\n",
    "app_infos_df = pd.DataFrame(app_infos)\n",
    "app_infos_df.to_csv('D:/Analyze the Data Series/Ep 2 Extracting and Scrapping/Play Store/Online Retail Indonesia/apps_info_indonesia.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapping app reviewsS\n",
    "\n",
    "app_reviews = []\n",
    "\n",
    "for ap in tqdm(app_packages):\n",
    "  for score in list(range(1, 6)):\n",
    "    for sort_order in [Sort.MOST_RELEVANT, Sort.NEWEST]:\n",
    "      rvs, _ = reviews(\n",
    "        ap,\n",
    "        country='id',\n",
    "        sort=sort_order,\n",
    "        count= 200 if score == 3 else 100, #Define Sample Size\n",
    "        filter_score_with=score\n",
    "      )\n",
    "      for r in rvs:\n",
    "        r['sortOrder'] = 'most_relevant' if sort_order == Sort.MOST_RELEVANT else 'newest'\n",
    "        r['appId'] = ap\n",
    "      app_reviews.extend(rvs)\n",
    " \n",
    "#source : https://towardsdatascience.com/create-dataset-for-sentiment-analysis-by-scraping-google-play-app-reviews-using-python-ceaaa0e41c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(app_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_json(app_reviews[12875])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract App Reviews to CSV\n",
    "app_reviews_df = pd.DataFrame(app_reviews)\n",
    "app_reviews_df.to_csv('D:/Analyze the Data Series/Ep 2 Extracting and Scrapping/Play Store/Online Retail Indonesia/reviews_online_retail_eng.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data \n",
    "app_reviews_df = pd.read_csv('D:/Analyze the Data Series/Ep 2 Extracting and Scrapping/Play Store/Online Retail Indonesia/reviews_online_retail_indonesia.csv')\n",
    "app_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Null Value\n",
    "print('Null Data:')\n",
    "print(app_reviews_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicate rows whose particular column is duplicate, the same people can't the same score, same review, and at the same time\n",
    "data1 = app_reviews_df.drop_duplicates(subset=['userName','at','score','content'])\n",
    "print('{:,} rows; {:,} columns'\n",
    "      .format(data1.shape[0], data1.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicate rows whose particular column is duplicate\n",
    "data2 = data1.drop_duplicates(subset=['content'])\n",
    "print('{:,} rows; {:,} columns'\n",
    "      .format(data2.shape[0], data2.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    #        Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def case_fold(text, title = None):\n",
    "        text = text.lower() #lowercase\n",
    "        text = re.sub(r\"\\d+\", \" \", text) #remove numbers\n",
    "        text = re.sub(r\"[,.;@#?!&$]+\\ *\", \" \", text) #renove punctuation\n",
    "        text = text.strip() #remove whitepace\n",
    "        text = stemmer.stem(text) #stemming\n",
    "        return text\n",
    "\n",
    "#source https://medium.com/@ksnugroho/dasar-text-preprocessing-dengan-python-a4fa52608ffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of stopwords, Adjust stopwords to your contents and your topics\n",
    "stopwords = set(get_stop_words('indonesian'))\n",
    "stopwords.update(get_stop_words('english'))\n",
    "stopwords.update([\"nya\",'yg','gak','ga','udah','gk','kalo','sy','ya','tdk','sya','pa','nih','uda','udh','kl','lg','lgi','koq',\n",
    "                  'tpi','tp','aja','ja','lbh','lbih','dr','dri','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p',\n",
    "                 'q','r','s','t','u','v','w','x','y','z','dah','gue','deh','sih','lo','banget','bgt','blm','sdh','klo','jgn',\n",
    "                  'jg','kya','gw','ngga','dpt','dpat','ko','sampe','smp','smpe','juga','gtu','bs','bsa','nggak','loh','ni','kak',\n",
    "                 'ehh','yaa','kaya','krna','krn','karna','jadi','untuk','terus'])\n",
    "stopwords.update(['aplikasi','mitra','seller','tokopedia','blibli','bukalapak','shopee','lazada','jdid','jd','id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust lowercase, remove numbers, punctuation, and stemming\n",
    "data2[\"processed\"] = data2[\"content\"].apply(case_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "data2['cleaned'] = data2['processed'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "print(data2[['appId','content','score','processed','cleaned']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract review after pre-processing text to CSV\n",
    "reviews_clean = data2[['appId','content','score','processed','cleaned']]\n",
    "reviews_clean_df = pd.DataFrame(reviews_clean)\n",
    "reviews_clean_df.to_csv('D:/Analyze the Data Series/Ep 2 Extracting and Scrapping/Play Store/Online Retail Indonesia/reviews_clean.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Word Frequency with Bar Chart and word Cloud\n",
    "#Update your stopwords by checking on these Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data \n",
    "reviews_clean_df = pd.read_csv('D:/Analyze the Data Series/Ep 2 Extracting and Scrapping/Play Store/Online Retail Indonesia/reviews_clean.csv')\n",
    "reviews_clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Cloud\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    text = \" \".join(review for review in data)\n",
    "    text = text.lower()\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(text))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud,interpolation='bilinear')\n",
    "    plt.show()\n",
    "\n",
    "show_wordcloud(reviews_clean_df['processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaned reviews\n",
    "product = reviews_clean_df['cleaned'].values.tolist()\n",
    "words_in_review = [word.split() for word in product]\n",
    "\n",
    "# List of all words across tweets\n",
    "all_words = [item for sublist in review_nsw for item in sublist]\n",
    "\n",
    "# Create counter\n",
    "import collections\n",
    "counts_words = collections.Counter(all_words)\n",
    "\n",
    "counts_words.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_words = pd.DataFrame(counts_words.most_common(15),\n",
    "                             columns=['words', 'count'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot horizontal bar graph\n",
    "most_words.sort_values(by='count').plot.barh(x='words',\n",
    "                      y='count',\n",
    "                      ax=ax,\n",
    "                      color=\"purple\")\n",
    "\n",
    "ax.set_title(\"Common Words Found in Review (Including All Words)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Reviews by Clustering the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import The Data\n",
    "reviews_clean_df = pd.read_csv('D:/Analyze the Data Series/Ep 2 Extracting and Scrapping/Play Store/Online Retail Indonesia/reviews_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example, I want to know what user dont like about the apps, so they gave 1 and 2 score for the review.\n",
    "neg_reviews = reviews_clean_df[reviews_clean_df[\"score\"].isin([1, 2])]\n",
    "neg_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Data:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'neg_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-86a5545d8e49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Check null rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Null Data:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_reviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'neg_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "#Check null rows\n",
    "print('Null Data:')\n",
    "print(neg_reviews.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to drop rows whose particular column is null, use this code\n",
    "data_neg = neg_reviews.dropna(subset=['cleaned'])\n",
    "print('{:,} rows; {:,} columns'\n",
    "      .format(data_neg.shape[0], data_neg.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf vectorizer of scikit learn\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data_neg['cleaned'].values.astype('U'))\n",
    "print(X.shape) # check shape of the document-term matrix\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the Optimal number of Cluster\n",
    "\n",
    "# Create empty sse dictionary\n",
    "sse = {}\n",
    "\n",
    "# Fit KMeans algorithm on k values between 1 and 11\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X)\n",
    "    sse[k] = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph of How to Find The Optimal Number of Cluster\n",
    "\n",
    "# Add the title to the plot\n",
    "plt.title('Elbow criterion method chart')\n",
    "\n",
    "# Create and display a scatter plot\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3 # Define the number of clusters based on the graph\n",
    "km = KMeans(n_clusters=num_clusters).fit(X)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying lsa //////////////////////////////\n",
    "# The words on clusters\n",
    "\n",
    "U, Sigma, VT = randomized_svd(X, n_components=num_clusters, n_iter=100,\n",
    "                              random_state=549)\n",
    "#printing the concepts\n",
    "for i, comp in enumerate(VT):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "        print(\"Concept \"+str(i)+\": \")\n",
    "        for t in sorted_terms:\n",
    "            print(t[0])\n",
    "        print(\" \")\n",
    "        \n",
    "#https://medium.com/kuzok/news-documents-clustering-using-python-latent-semantic-analysis-b95c7b68861c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge data with label\n",
    "label = km.labels_\n",
    "label = pd.DataFrame(label)\n",
    "\n",
    "df_label_neg = pd.concat([data_neg.reset_index(drop=True), label], axis=1)\n",
    "df_label_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_label_neg.shape)\n",
    "print(label[0].value_counts()) # Count the members of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract File into CSV\n",
    "df_label_neg.to_csv('D:/Analyze the Data Series/Ep 2 Extracting and Scrapping/Play Store/Online Retail Indonesia/label_neg.csv', index=None, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
